{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chicago Crime Analysis - Model Training, Evaluation, and Results\n",
    "\n",
    "This notebook focuses on training and evaluating our models, comparing their performance, and producing final results and visualizations. Building on notebooks 1 and 2, we'll finalize our analysis of Chicago theft crimes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, roc_curve, roc_auc_score,\n",
    "                             precision_recall_curve, auc, mean_squared_error, mean_absolute_error, r2_score)\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import joblib\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "# Add project root to path for imports\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "from src.models.ml_models import RandomForestModel, GradientBoostingModel\n",
    "from src.evaluation.metrics import classification_metrics, regression_metrics, error_analysis\n",
    "from src.visualization.visualize import CrimeDataVisualization\n",
    "from src.utils import align_features\n",
    "\n",
    "# Set plotting style\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.rcParams.update({'font.size': 12})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Prepared Data\n",
    "\n",
    "First, we'll load the processed data we prepared in notebook 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load classification data\n",
    "processed_dir = 'data/processed'\n",
    "try:\n",
    "    classification_data = joblib.load(os.path.join(processed_dir, 'classification_data.joblib'))\n",
    "    \n",
    "    # Load time series data\n",
    "    ts_data_weekly = joblib.load(os.path.join(processed_dir, 'time_series_weekly_data.joblib'))\n",
    "    ts_data_monthly = joblib.load(os.path.join(processed_dir, 'time_series_monthly_data.joblib'))\n",
    "\n",
    "    X_train = classification_data['X_train']\n",
    "    X_test = classification_data['X_test']\n",
    "    y_train = classification_data['y_train']\n",
    "    y_test = classification_data['y_test']\n",
    "    X_train_balanced = classification_data.get('X_train_combined', classification_data['X_train'])\n",
    "    y_train_balanced = classification_data.get('y_train_combined', classification_data['y_train'])\n",
    "    feature_names = classification_data['feature_names']\n",
    "    \n",
    "    print(\"Data loaded successfully:\")\n",
    "    print(f\"Classification data: X_train shape: {X_train.shape}, X_test shape: {X_test.shape}\")\n",
    "    \n",
    "    print(f\"Time series weekly data: X_train shape: {ts_data_weekly['X_train'].shape}, X_test shape: {ts_data_weekly['X_test'].shape}\")\n",
    "    print(f\"Time series monthly data: X_train shape: {ts_data_monthly['X_train'].shape}, X_test shape: {ts_data_monthly['X_test'].shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading classification and Time Series data: {e}\")\n",
    "    raise\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train.value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Traditional Machine Learning Models for Classification\n",
    "\n",
    "Let's train and optimize our Random Forest and Gradient Boosting models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_features(model, X_test):\n",
    "    \"\"\"\n",
    "    Align features between model's training data and test data\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : sklearn estimator\n",
    "        Trained model\n",
    "    X_test : pandas DataFrame\n",
    "        Test features\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    X_test_aligned : aligned DataFrame\n",
    "    \"\"\"\n",
    "    # Get feature names used during training\n",
    "    if hasattr(model, 'feature_names_in_'):\n",
    "        train_features = model.feature_names_in_\n",
    "    else:\n",
    "        raise ValueError(\"Cannot retrieve feature names from the model\")\n",
    "    \n",
    "    # Identify common features\n",
    "    test_features = X_test.columns.tolist()\n",
    "    \n",
    "    # Find missing and extra features\n",
    "    missing_features = set(train_features) - set(test_features)\n",
    "    extra_features = set(test_features) - set(train_features)\n",
    "    \n",
    "    # Print out feature mismatches for debugging\n",
    "    if missing_features:\n",
    "        print(\"Features in training but missing in test:\")\n",
    "        print(missing_features)\n",
    "    \n",
    "    if extra_features:\n",
    "        print(\"Features in test but not in training:\")\n",
    "        print(extra_features)\n",
    "    \n",
    "    # Create aligned test dataset\n",
    "    X_test_aligned = X_test[list(set(train_features) & set(test_features))]\n",
    "    \n",
    "    # Add missing columns with zeros if needed\n",
    "    for feature in missing_features:\n",
    "        X_test_aligned[feature] = 0\n",
    "    \n",
    "    # Reorder columns to match training order\n",
    "    X_test_aligned = X_test_aligned[train_features]\n",
    "    \n",
    "    return X_test_aligned\n",
    "\n",
    "def evaluate_classifier(model, X_test, y_test, model_name):\n",
    "    # Align features first\n",
    "    try:\n",
    "        X_test_aligned = align_features(model, X_test)\n",
    "    except Exception as e:\n",
    "        print(f\"Feature alignment error: {e}\")\n",
    "        raise\n",
    "    \n",
    "    # Make predictions\n",
    "    start_time = time.time()\n",
    "    # Step 1: Get predicted probabilities for class 1\n",
    "    y_probs = model.predict_proba(X_test_aligned)[:, 1]\n",
    "\n",
    "    # Step 2: Apply custom threshold (e.g., 0.3 or dynamically determined)\n",
    "    threshold = 0.2\n",
    "    y_pred = (y_probs > threshold).astype(int)\n",
    "\n",
    "    inference_time = time.time() - start_time\n",
    "    \n",
    "    # Get probabilities for ROC curve\n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        y_proba = model.predict_proba(X_test_aligned)[:, 1]\n",
    "    else:\n",
    "        y_proba = (y_probs > threshold).astype(int)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = classification_metrics(y_test, y_pred, y_proba)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\n{model_name} Results:\")\n",
    "    print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
    "    print(f\"Precision: {metrics['precision']:.4f}\")\n",
    "    print(f\"Recall: {metrics['recall']:.4f}\")\n",
    "    print(f\"F1 Score: {metrics['f1_score']:.4f}\")\n",
    "    print(f\"AUC: {metrics.get('auc', 'N/A')}\")\n",
    "    print(f\"Inference time: {inference_time:.6f} seconds\")\n",
    "    \n",
    "    # Calculate confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    return metrics, y_pred, y_proba, cm, inference_time\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning for Random Forest\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'class_weight': ['balanced', 'balanced_subsample']\n",
    "}\n",
    "\n",
    "rf_model = RandomForestModel()\n",
    "rf = rf_model.build().model\n",
    "\n",
    "print(\"Performing grid search for Random Forest...\")\n",
    "try:\n",
    "    grid_search_rf = GridSearchCV(rf, param_grid_rf, cv=5, scoring='f1', n_jobs=-1)\n",
    "    grid_search_rf.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "    print(f\"Best parameters: {grid_search_rf.best_params_}\")\n",
    "    print(f\"Best cross-validation score: {grid_search_rf.best_score_:.4f}\")\n",
    "\n",
    "    # Use the best model\n",
    "    rf_best = grid_search_rf.best_estimator_\n",
    "\n",
    "    # Evaluate on test set\n",
    "    rf_metrics, rf_pred, rf_proba, rf_cm, rf_time = evaluate_classifier(rf_best, X_test, y_test, \"Random Forest (Optimized)\")\n",
    "except Exception as e:\n",
    "    print(f\"Error in Random Forest grid search: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning for Gradient Boosting\n",
    "param_grid_gb = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'max_depth': [3, 4, 5],\n",
    "    'subsample': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "gb_model = GradientBoostingModel()\n",
    "gb = gb_model.build().model\n",
    "\n",
    "print(\"Performing grid search for Gradient Boosting...\")\n",
    "grid_search_gb = GridSearchCV(gb, param_grid_gb, cv=5, scoring='f1', n_jobs=-1)\n",
    "grid_search_gb.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "print(f\"Best parameters: {grid_search_gb.best_params_}\")\n",
    "print(f\"Best cross-validation score: {grid_search_gb.best_score_:.4f}\")\n",
    "\n",
    "# Use the best model\n",
    "gb_best = grid_search_gb.best_estimator_\n",
    "\n",
    "# Evaluate on test set\n",
    "gb_metrics, gb_pred, gb_proba, gb_cm, gb_time = evaluate_classifier(gb_best, X_test, y_test, \"Gradient Boosting (Optimized)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualizations for model comparison\n",
    "# ROC curves\n",
    "plt.figure(figsize=(10, 8))\n",
    "fpr_rf, tpr_rf, _ = roc_curve(y_test, rf_proba)\n",
    "fpr_gb, tpr_gb, _ = roc_curve(y_test, gb_proba)\n",
    "\n",
    "plt.plot(fpr_rf, tpr_rf, label=f'Random Forest (AUC = {rf_metrics[\"auc\"]:.4f})')\n",
    "plt.plot(fpr_gb, tpr_gb, label=f'Gradient Boosting (AUC = {gb_metrics[\"auc\"]:.4f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves for Classification Models')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.savefig('reports/figures/roc_curves_traditional_ml.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Confusion matrices\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "sns.heatmap(rf_cm, annot=True, fmt='d', cmap='Blues', ax=ax1)\n",
    "ax1.set_title('Random Forest Confusion Matrix')\n",
    "ax1.set_xlabel('Predicted Label')\n",
    "ax1.set_ylabel('True Label')\n",
    "\n",
    "sns.heatmap(gb_cm, annot=True, fmt='d', cmap='Blues', ax=ax2)\n",
    "ax2.set_title('Gradient Boosting Confusion Matrix')\n",
    "ax2.set_xlabel('Predicted Label')\n",
    "ax2.set_ylabel('True Label')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('reports/figures/confusion_matrices_traditional_ml.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if hasattr(gb_best, 'feature_importances_'):\n",
    "    # Get feature importances\n",
    "    importances = gb_best.feature_importances_\n",
    "    \n",
    "    try:\n",
    "        # Approach 1: Try to get feature names from the model\n",
    "        if hasattr(gb_best, 'feature_names_in_'):\n",
    "            print(\"Using feature names from the model\")\n",
    "            model_feature_names = gb_best.feature_names_in_\n",
    "            \n",
    "            # Create DataFrame with model's feature names\n",
    "            feature_importance = pd.DataFrame({\n",
    "                'Feature': model_feature_names,\n",
    "                'Importance': importances\n",
    "            }).sort_values('Importance', ascending=False)\n",
    "        \n",
    "        # Approach 2: Use provided feature_names if lengths match\n",
    "        elif len(feature_names) == len(importances):\n",
    "            print(\"Using provided feature_names\")\n",
    "            feature_importance = pd.DataFrame({\n",
    "                'Feature': feature_names,\n",
    "                'Importance': importances\n",
    "            }).sort_values('Importance', ascending=False)\n",
    "        \n",
    "        # Approach 3: Truncate to common length\n",
    "        else:\n",
    "            print(f\"Lengths don't match: feature_names={len(feature_names)}, importances={len(importances)}\")\n",
    "            print(\"Creating dataframe with minimum common length\")\n",
    "            \n",
    "            min_length = min(len(feature_names), len(importances))\n",
    "            feature_importance = pd.DataFrame({\n",
    "                'Feature': feature_names[:min_length],\n",
    "                'Importance': importances[:min_length]\n",
    "            }).sort_values('Importance', ascending=False)\n",
    "        \n",
    "        # Plot top features (up to 15)\n",
    "        top_n = min(15, len(feature_importance))\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        sns.barplot(x='Importance', y='Feature', data=feature_importance.head(top_n))\n",
    "        plt.title(f'Top {top_n} Feature Importances for Predicting Arrests (Gradient Boosting)')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('reports/figures/feature_importance_gb.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        # Save feature importances for later reference\n",
    "        feature_importance.to_csv('reports/feature_importance_gb.csv', index=False)\n",
    "        \n",
    "        # Print top features (up to 10)\n",
    "        top_n_print = min(10, len(feature_importance))\n",
    "        print(f\"Top {top_n_print} most important features:\")\n",
    "        print(feature_importance.head(top_n_print))\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating feature importance visualization: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "else:\n",
    "    print(\"Model doesn't have feature_importances_ attribute\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Deep Learning Models for Classification\n",
    "\n",
    "Now, let's train and evaluate our MLP model for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Simple preprocessing function\n",
    "def preprocess_for_torch(X, y=None):\n",
    "    \"\"\"Basic preprocessing for PyTorch tensors\"\"\"\n",
    "    # Convert to numpy array\n",
    "    if isinstance(X, pd.DataFrame) or isinstance(X, pd.Series):\n",
    "        X_array = X.values.astype(np.float32)\n",
    "    else:\n",
    "        X_array = np.asarray(X, dtype=np.float32)\n",
    "    \n",
    "    if y is not None:\n",
    "        if isinstance(y, pd.DataFrame) or isinstance(y, pd.Series):\n",
    "            y_array = y.values.astype(np.float32)\n",
    "        else:\n",
    "            y_array = np.asarray(y, dtype=np.float32)\n",
    "        return X_array, y_array\n",
    "    \n",
    "    return X_array\n",
    "\n",
    "# Print original shapes\n",
    "print(\"Original shapes:\")\n",
    "print(f\"X_train_balanced: {X_train_balanced.shape}\")\n",
    "print(f\"X_test: {X_test.shape}\")\n",
    "\n",
    "# Try to find common features\n",
    "if isinstance(X_train_balanced, pd.DataFrame) and isinstance(X_test, pd.DataFrame):\n",
    "    # Check for common columns\n",
    "    common_cols = list(set(X_train_balanced.columns) & set(X_test.columns))\n",
    "    print(f\"Found {len(common_cols)} common columns\")\n",
    "    \n",
    "    if len(common_cols) > 0:\n",
    "        # Use only common columns\n",
    "        X_train_subset = X_train_balanced[common_cols]\n",
    "        X_test_subset = X_test[common_cols]\n",
    "    else:\n",
    "        # No common columns, try to use numeric columns\n",
    "        print(\"No common columns! Using numeric features only.\")\n",
    "        X_train_subset = X_train_balanced.select_dtypes(include=['number'])\n",
    "        X_test_subset = X_test.select_dtypes(include=['number'])\n",
    "        \n",
    "        # Get common numeric columns\n",
    "        common_numeric_cols = list(set(X_train_subset.columns) & set(X_test_subset.columns))\n",
    "        print(f\"Found {len(common_numeric_cols)} common numeric columns\")\n",
    "        \n",
    "        if len(common_numeric_cols) > 0:\n",
    "            X_train_subset = X_train_subset[common_numeric_cols]\n",
    "            X_test_subset = X_test_subset[common_numeric_cols]\n",
    "        else:\n",
    "            # We're out of options, recreate features from scratch\n",
    "            print(\"No common numeric columns either. Creating simplified features.\")\n",
    "            \n",
    "            # Create dummy features\n",
    "            X_train_subset = pd.DataFrame({\n",
    "                'feature1': np.random.rand(len(X_train_balanced)),\n",
    "                'feature2': np.random.rand(len(X_train_balanced))\n",
    "            })\n",
    "            \n",
    "            X_test_subset = pd.DataFrame({\n",
    "                'feature1': np.random.rand(len(X_test)),\n",
    "                'feature2': np.random.rand(len(X_test))\n",
    "            })\n",
    "else:\n",
    "    # For numpy arrays\n",
    "    X_train_subset = X_train_balanced\n",
    "    X_test_subset = X_test[:, :X_train_balanced.shape[1]]  # Use only the first N columns from X_test\n",
    "\n",
    "print(f\"Aligned X_train_subset shape: {X_train_subset.shape}\")\n",
    "print(f\"Aligned X_test_subset shape: {X_test_subset.shape}\")\n",
    "\n",
    "# Preprocess the aligned data\n",
    "X_train_proc, y_train_proc = preprocess_for_torch(X_train_subset, y_train_balanced)\n",
    "X_test_proc, y_test_proc = preprocess_for_torch(X_test_subset, y_test)\n",
    "\n",
    "print(f\"Processed X_train shape: {X_train_proc.shape}\")\n",
    "print(f\"Processed X_test shape: {X_test_proc.shape}\")\n",
    "\n",
    "# Verify dimensions match\n",
    "if X_train_proc.shape[1] != X_test_proc.shape[1]:\n",
    "    print(\"ERROR: Feature dimensions still don't match after alignment!\")\n",
    "    print(\"Forcing dimensions to match by truncating...\")\n",
    "    min_features = min(X_train_proc.shape[1], X_test_proc.shape[1])\n",
    "    X_train_proc = X_train_proc[:, :min_features]\n",
    "    X_test_proc = X_test_proc[:, :min_features]\n",
    "    \n",
    "    print(f\"Truncated X_train shape: {X_train_proc.shape}\")\n",
    "    print(f\"Truncated X_test shape: {X_test_proc.shape}\")\n",
    "\n",
    "# Convert to tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train_proc).to(device)\n",
    "y_train_tensor = torch.FloatTensor(y_train_proc).reshape(-1, 1).to(device)\n",
    "X_test_tensor = torch.FloatTensor(X_test_proc).to(device)\n",
    "y_test_tensor = torch.FloatTensor(y_test_proc).reshape(-1, 1).to(device)\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# Define the MLP model\n",
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims=[128, 64], dropout_rate=0.3):\n",
    "        super(MLPClassifier, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        print(f\"Creating model with input_dim={input_dim}, hidden_dims={hidden_dims}\")\n",
    "        \n",
    "        # Input layer\n",
    "        self.layers.append(nn.Linear(input_dim, hidden_dims[0]))\n",
    "        self.layers.append(nn.ReLU())\n",
    "        self.layers.append(nn.BatchNorm1d(hidden_dims[0]))\n",
    "        self.layers.append(nn.Dropout(dropout_rate))\n",
    "        \n",
    "        # Hidden layers\n",
    "        for i in range(len(hidden_dims) - 1):\n",
    "            self.layers.append(nn.Linear(hidden_dims[i], hidden_dims[i+1]))\n",
    "            self.layers.append(nn.ReLU())\n",
    "            self.layers.append(nn.BatchNorm1d(hidden_dims[i+1]))\n",
    "            self.layers.append(nn.Dropout(dropout_rate))\n",
    "        \n",
    "        # Output layer\n",
    "        self.layers.append(nn.Linear(hidden_dims[-1], 1))\n",
    "        self.layers.append(nn.Sigmoid())\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            try:\n",
    "                x = layer(x)\n",
    "            except Exception as e:\n",
    "                print(f\"Error in layer {i}: {type(layer).__name__}\")\n",
    "                print(f\"Input shape: {x.shape}\")\n",
    "                print(f\"Layer info: {layer}\")\n",
    "                raise\n",
    "        return x\n",
    "\n",
    "# Initialize model with the correct input dimension\n",
    "input_dim = X_train_tensor.shape[1]  # Use the actual tensor shape\n",
    "print(f\"Using input dimension: {input_dim}\")\n",
    "hidden_dims = [128, 64]  # Can be tuned\n",
    "dropout_rate = 0.3  # Can be tuned\n",
    "\n",
    "mlp_model = MLPClassifier(input_dim=input_dim, hidden_dims=hidden_dims, dropout_rate=dropout_rate).to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(mlp_model.parameters(), lr=0.001)\n",
    "\n",
    "try:\n",
    "    # Training loop\n",
    "    num_epochs = 100\n",
    "    training_losses = []\n",
    "    validation_losses = []\n",
    "\n",
    "    print(\"Training MLP model...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        mlp_model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        for batch_idx, (X_batch, y_batch) in enumerate(train_loader):\n",
    "            try:\n",
    "                # Print shape of first batch for debugging\n",
    "                if epoch == 0 and batch_idx == 0:\n",
    "                    print(f\"First batch shapes - X: {X_batch.shape}, y: {y_batch.shape}\")\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = mlp_model(X_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                \n",
    "                # Backward and optimize\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                train_loss += loss.item() * X_batch.size(0)\n",
    "            except Exception as e:\n",
    "                print(f\"Error in batch {batch_idx}: {e}\")\n",
    "                print(f\"X_batch shape: {X_batch.shape}\")\n",
    "                raise\n",
    "        \n",
    "        train_loss /= len(train_loader.dataset)\n",
    "        training_losses.append(train_loss)\n",
    "        \n",
    "        # Validation\n",
    "        mlp_model.eval()\n",
    "        val_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in test_loader:\n",
    "                outputs = mlp_model(X_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                val_loss += loss.item() * X_batch.size(0)\n",
    "        \n",
    "        val_loss /= len(test_loader.dataset)\n",
    "        validation_losses.append(val_loss)\n",
    "        \n",
    "        # Print progress\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "\n",
    "    training_time = time.time() - start_time\n",
    "    print(f\"Training completed in {training_time:.2f} seconds\")\n",
    "\n",
    "    # Plot training and validation loss\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, num_epochs + 1), training_losses, label='Training Loss')\n",
    "    plt.plot(range(1, num_epochs + 1), validation_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('MLP Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.savefig('reports/figures/mlp_loss_curves.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print(f\"Training error: {e}\")\n",
    "    print(\"Model architecture:\")\n",
    "    print(mlp_model)\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate MLP model\n",
    "def evaluate_mlp(model, test_loader, threshold=0.5):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_probs = []\n",
    "    all_labels = []\n",
    "    \n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            outputs = model(X_batch)\n",
    "            probabilities = outputs.cpu().numpy()\n",
    "            predictions = (outputs >= threshold).float().cpu().numpy()\n",
    "            labels = y_batch.cpu().numpy()\n",
    "            \n",
    "            all_preds.extend(predictions)\n",
    "            all_probs.extend(probabilities)\n",
    "            all_labels.extend(labels)\n",
    "    \n",
    "    inference_time = time.time() - start_time\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    all_preds = np.array(all_preds).flatten()\n",
    "    all_probs = np.array(all_probs).flatten()\n",
    "    all_labels = np.array(all_labels).flatten()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = classification_metrics(all_labels, all_preds, all_probs)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\nMLP Model Results:\")\n",
    "    print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
    "    print(f\"Precision: {metrics['precision']:.4f}\")\n",
    "    print(f\"Recall: {metrics['recall']:.4f}\")\n",
    "    print(f\"F1 Score: {metrics['f1_score']:.4f}\")\n",
    "    print(f\"AUC: {metrics.get('auc', 'N/A')}\")\n",
    "    print(f\"Inference time: {inference_time:.6f} seconds (for entire test set)\")\n",
    "    \n",
    "    # Calculate confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    \n",
    "    return metrics, all_preds, all_probs, cm, inference_time\n",
    "\n",
    "mlp_metrics, mlp_pred, mlp_proba, mlp_cm, mlp_time = evaluate_mlp(mlp_model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all classification models\n",
    "# Plot ROC curves\n",
    "plt.figure(figsize=(10, 8))\n",
    "fpr_rf, tpr_rf, _ = roc_curve(y_test, rf_proba)\n",
    "fpr_gb, tpr_gb, _ = roc_curve(y_test, gb_proba)\n",
    "fpr_mlp, tpr_mlp, _ = roc_curve(y_test, mlp_proba)\n",
    "\n",
    "plt.plot(fpr_rf, tpr_rf, label=f'Random Forest (AUC = {rf_metrics[\"auc\"]:.4f})')\n",
    "plt.plot(fpr_gb, tpr_gb, label=f'Gradient Boosting (AUC = {gb_metrics[\"auc\"]:.4f})')\n",
    "plt.plot(fpr_mlp, tpr_mlp, label=f'MLP (AUC = {mlp_metrics[\"auc\"]:.4f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves Comparison')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.savefig('reports/figures/roc_curves_all_models.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Create a comparison table\n",
    "comparison_data = {\n",
    "    'Model': ['Random Forest', 'Gradient Boosting', 'MLP'],\n",
    "    'Accuracy': [rf_metrics['accuracy'], gb_metrics['accuracy'], mlp_metrics['accuracy']],\n",
    "    'Precision': [rf_metrics['precision'], gb_metrics['precision'], mlp_metrics['precision']],\n",
    "    'Recall': [rf_metrics['recall'], gb_metrics['recall'], mlp_metrics['recall']],\n",
    "    'F1 Score': [rf_metrics['f1_score'], gb_metrics['f1_score'], mlp_metrics['f1_score']],\n",
    "    'AUC': [rf_metrics['auc'], gb_metrics['auc'], mlp_metrics['auc']],\n",
    "    'Training Time (s)': ['N/A', 'N/A', training_time],\n",
    "    'Inference Time (s)': [rf_time, gb_time, mlp_time]\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "comparison_df.set_index('Model', inplace=True)\n",
    "\n",
    "# Display formatted table\n",
    "print(comparison_df.round(4))\n",
    "\n",
    "# Save comparison table\n",
    "comparison_df.to_csv('reports/classification_model_comparison.csv')\n",
    "\n",
    "# Visualize comparison\n",
    "metrics_to_plot = ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'AUC']\n",
    "comparison_plot_data = comparison_df[metrics_to_plot].reset_index()\n",
    "comparison_plot_data_melted = pd.melt(comparison_plot_data, id_vars='Model', var_name='Metric', value_name='Value')\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='Metric', y='Value', hue='Model', data=comparison_plot_data_melted)\n",
    "plt.title('Classification Model Comparison')\n",
    "plt.ylim(0, 1)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend(title='Model')\n",
    "plt.tight_layout()\n",
    "plt.savefig('reports/figures/classification_model_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. LSTM Model for Time Series Prediction\n",
    "\n",
    "Now, let's train and evaluate our LSTM model for predicting crime counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the LSTM model\n",
    "class LSTMPredictor(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_size=64, num_layers=2, dropout=0.2):\n",
    "        super(LSTMPredictor, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state with zeros\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        # Forward propagate LSTM\n",
    "        out, _ = self.lstm(x, (h0, c0))  # out: batch_size, seq_length, hidden_size\n",
    "        \n",
    "        # Decode the hidden state of the last time step\n",
    "        out = self.fc(out[:, -1, :])  \n",
    "        return out\n",
    "\n",
    "# Prepare data for LSTM model\n",
    "X_train_ts = torch.FloatTensor(ts_data_weekly['X_train']).to(device)\n",
    "y_train_ts = torch.FloatTensor(ts_data_weekly['y_train'].reshape(-1, 1)).to(device)\n",
    "X_test_ts = torch.FloatTensor(ts_data_weekly['X_test']).to(device)\n",
    "y_test_ts = torch.FloatTensor(ts_data_weekly['y_test'].reshape(-1, 1)).to(device)\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_ts_dataset = TensorDataset(X_train_ts, y_train_ts)\n",
    "test_ts_dataset = TensorDataset(X_test_ts, y_test_ts)\n",
    "\n",
    "batch_size_ts = 16\n",
    "train_ts_loader = DataLoader(train_ts_dataset, batch_size=batch_size_ts, shuffle=True)\n",
    "test_ts_loader = DataLoader(test_ts_dataset, batch_size=batch_size_ts)\n",
    "\n",
    "# Initialize LSTM model\n",
    "# Train LSTM model\n",
    "try:\n",
    "    # Initialize model\n",
    "    input_size = X_train_ts.shape[2]  # Number of features (1 for univariate time series)\n",
    "    hidden_size = 64\n",
    "    num_layers = 2\n",
    "    dropout = 0.2\n",
    "\n",
    "    lstm_model = LSTMPredictor(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=dropout).to(device)\n",
    "\n",
    "    # Loss function and optimizer\n",
    "    criterion_ts = nn.MSELoss()\n",
    "    optimizer_ts = optim.Adam(lstm_model.parameters(), lr=0.001)\n",
    "\n",
    "    # Training loop\n",
    "    num_epochs_ts = 100\n",
    "    ts_training_losses = []\n",
    "    ts_validation_losses = []\n",
    "\n",
    "    print(\"Training LSTM model for time series prediction...\")\n",
    "    ts_start_time = time.time()\n",
    "\n",
    "    for epoch in range(num_epochs_ts):\n",
    "        # Training\n",
    "        lstm_model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        for X_batch, y_batch in train_ts_loader:\n",
    "            # Forward pass\n",
    "            outputs = lstm_model(X_batch)\n",
    "            loss = criterion_ts(outputs, y_batch)\n",
    "            \n",
    "            # Backward and optimize\n",
    "            optimizer_ts.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer_ts.step()\n",
    "            \n",
    "            train_loss += loss.item() * X_batch.size(0)\n",
    "        \n",
    "        train_loss /= len(train_ts_loader.dataset)\n",
    "        ts_training_losses.append(train_loss)\n",
    "        \n",
    "        # Validation\n",
    "        lstm_model.eval()\n",
    "        val_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in test_ts_loader:\n",
    "                outputs = lstm_model(X_batch)\n",
    "                loss = criterion_ts(outputs, y_batch)\n",
    "                val_loss += loss.item() * X_batch.size(0)\n",
    "        \n",
    "        val_loss /= len(test_ts_loader.dataset)\n",
    "        ts_validation_losses.append(val_loss)\n",
    "        \n",
    "        # Print progress\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs_ts}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "\n",
    "    ts_training_time = time.time() - ts_start_time\n",
    "    print(f\"Training completed in {ts_training_time:.2f} seconds\")\n",
    "except Exception as e:\n",
    "    print(f\"Error in LSTM model training: {e}\")\n",
    "    raise\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, num_epochs_ts + 1), ts_training_losses, label='Training Loss')\n",
    "plt.plot(range(1, num_epochs_ts + 1), ts_validation_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('LSTM Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.savefig('reports/figures/lstm_loss_curves.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate LSTM model\n",
    "def evaluate_lstm(model, test_loader, scaler):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    \n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            outputs = model(X_batch)\n",
    "            predictions.append(outputs.cpu().numpy())\n",
    "            actuals.append(y_batch.cpu().numpy())\n",
    "    \n",
    "    inference_time = time.time() - start_time\n",
    "    \n",
    "    # Concatenate batches\n",
    "    predictions = np.concatenate(predictions)\n",
    "    actuals = np.concatenate(actuals)\n",
    "    \n",
    "    # Inverse transform to original scale\n",
    "    if scaler:\n",
    "        predictions = scaler.inverse_transform(predictions)\n",
    "        actuals = scaler.inverse_transform(actuals)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mse = mean_squared_error(actuals, predictions)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(actuals, predictions)\n",
    "    r2 = r2_score(actuals, predictions)\n",
    "    \n",
    "    # Calculate MAPE\n",
    "    mask = actuals != 0\n",
    "    mape = np.mean(np.abs((actuals[mask] - predictions[mask]) / actuals[mask])) * 100\n",
    "    \n",
    "    print(f\"\\nLSTM Time Series Model Results:\")\n",
    "    print(f\"MSE: {mse:.4f}\")\n",
    "    print(f\"RMSE: {rmse:.4f}\")\n",
    "    print(f\"MAE: {mae:.4f}\")\n",
    "    print(f\"R²: {r2:.4f}\")\n",
    "    print(f\"MAPE: {mape:.2f}%\")\n",
    "    print(f\"Inference time: {inference_time:.6f} seconds (for entire test set)\")\n",
    "    \n",
    "    return {\n",
    "        'mse': mse,\n",
    "        'rmse': rmse,\n",
    "        'mae': mae,\n",
    "        'r2': r2,\n",
    "        'mape': mape\n",
    "    }, predictions.flatten(), actuals.flatten(), inference_time\n",
    "\n",
    "# Evaluate model\n",
    "lstm_metrics, lstm_pred, lstm_actual, lstm_inference_time = evaluate_lstm(lstm_model, test_ts_loader, ts_data_weekly['scaler'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with traditional time series methods (ARIMA and Prophet)\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "import pandas as pd\n",
    "from prophet import Prophet\n",
    "\n",
    "# Get the original data\n",
    "ts_data = ts_data_weekly['original_data']\n",
    "train_size = int(len(ts_data) * 0.8)\n",
    "train_data = ts_data.iloc[:train_size]\n",
    "test_data = ts_data.iloc[train_size:]\n",
    "\n",
    "# ARIMA model\n",
    "print(\"Training ARIMA model...\")\n",
    "arima_start_time = time.time()\n",
    "arima_model = ARIMA(train_data['crime_count'], order=(2, 1, 2))\n",
    "arima_model_fit = arima_model.fit()\n",
    "arima_training_time = time.time() - arima_start_time\n",
    "\n",
    "# Make predictions\n",
    "arima_pred_start = time.time()\n",
    "arima_predictions = arima_model_fit.forecast(steps=len(test_data))\n",
    "arima_inference_time = time.time() - arima_pred_start\n",
    "\n",
    "# Prophet model\n",
    "print(\"\\nTraining Prophet model...\")\n",
    "prophet_data = train_data.rename(columns={'date': 'ds', 'crime_count': 'y'})\n",
    "prophet_model = Prophet()\n",
    "prophet_start_time = time.time()\n",
    "prophet_model.fit(prophet_data)\n",
    "prophet_training_time = time.time() - prophet_start_time\n",
    "\n",
    "# Create future dataframe\n",
    "future = prophet_model.make_future_dataframe(periods=len(test_data), freq='W')\n",
    "# Make predictions\n",
    "prophet_pred_start = time.time()\n",
    "prophet_forecast = prophet_model.predict(future)\n",
    "prophet_inference_time = time.time() - prophet_pred_start\n",
    "prophet_predictions = prophet_forecast.iloc[-len(test_data):]['yhat'].values\n",
    "\n",
    "# Evaluate ARIMA\n",
    "arima_mse = mean_squared_error(test_data['crime_count'], arima_predictions)\n",
    "arima_rmse = np.sqrt(arima_mse)\n",
    "arima_mae = mean_absolute_error(test_data['crime_count'], arima_predictions)\n",
    "arima_r2 = r2_score(test_data['crime_count'], arima_predictions)\n",
    "# MAPE\n",
    "mask = test_data['crime_count'] != 0\n",
    "arima_mape = np.mean(np.abs((test_data['crime_count'][mask].values - arima_predictions[mask]) / test_data['crime_count'][mask].values)) * 100\n",
    "\n",
    "print(\"\\nARIMA Model Results:\")\n",
    "print(f\"MSE: {arima_mse:.4f}\")\n",
    "print(f\"RMSE: {arima_rmse:.4f}\")\n",
    "print(f\"MAE: {arima_mae:.4f}\")\n",
    "print(f\"R²: {arima_r2:.4f}\")\n",
    "print(f\"MAPE: {arima_mape:.2f}%\")\n",
    "print(f\"Training time: {arima_training_time:.6f} seconds\")\n",
    "print(f\"Inference time: {arima_inference_time:.6f} seconds\")\n",
    "\n",
    "# Evaluate Prophet\n",
    "prophet_mse = mean_squared_error(test_data['crime_count'], prophet_predictions)\n",
    "prophet_rmse = np.sqrt(prophet_mse)\n",
    "prophet_mae = mean_absolute_error(test_data['crime_count'], prophet_predictions)\n",
    "prophet_r2 = r2_score(test_data['crime_count'], prophet_predictions)\n",
    "# MAPE\n",
    "mask = test_data['crime_count'] != 0\n",
    "prophet_mape = np.mean(np.abs((test_data['crime_count'][mask].values - prophet_predictions[mask]) / test_data['crime_count'][mask].values)) * 100\n",
    "\n",
    "print(\"\\nProphet Model Results:\")\n",
    "print(f\"MSE: {prophet_mse:.4f}\")\n",
    "print(f\"RMSE: {prophet_rmse:.4f}\")\n",
    "print(f\"MAE: {prophet_mae:.4f}\")\n",
    "print(f\"R²: {prophet_r2:.4f}\")\n",
    "print(f\"MAPE: {prophet_mape:.2f}%\")\n",
    "print(f\"Training time: {prophet_training_time:.6f} seconds\")\n",
    "print(f\"Inference time: {prophet_inference_time:.6f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug the shapes\n",
    "print(f\"test_data.index length: {len(test_data.index)}\")\n",
    "print(f\"test_data['crime_count'] length: {len(test_data['crime_count'])}\")\n",
    "print(f\"lstm_pred length: {len(lstm_pred)}\")\n",
    "print(f\"arima_predictions length: {len(arima_predictions)}\")\n",
    "print(f\"prophet_predictions length: {len(prophet_predictions)}\")\n",
    "\n",
    "# Find the minimum length they all share\n",
    "min_length = min(\n",
    "    len(test_data.index),\n",
    "    len(test_data['crime_count']),\n",
    "    len(lstm_pred),\n",
    "    len(arima_predictions),\n",
    "    len(prophet_predictions)\n",
    ")\n",
    "\n",
    "print(f\"Using minimum length of {min_length} for plotting\")\n",
    "\n",
    "# Truncate all arrays to this common length\n",
    "index_aligned = test_data.index[:min_length]\n",
    "actual_aligned = test_data['crime_count'].values[:min_length]\n",
    "lstm_aligned = lstm_pred[:min_length]\n",
    "arima_aligned = arima_predictions[:min_length]\n",
    "prophet_aligned = prophet_predictions[:min_length]\n",
    "\n",
    "# Plot the aligned data\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(index_aligned, actual_aligned, label='Actual', marker='o')\n",
    "plt.plot(index_aligned, lstm_aligned, label='LSTM', marker='x')\n",
    "plt.plot(index_aligned, arima_aligned, label='ARIMA', marker='s')\n",
    "plt.plot(index_aligned, prophet_aligned, label='Prophet', marker='d')\n",
    "plt.title('Time Series Model Predictions vs Actual Crime Counts')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Crime Count')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('reports/figures/time_series_predictions.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Calculate metrics on aligned data\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "def calculate_metrics(actual, predicted):\n",
    "    mse = mean_squared_error(actual, predicted)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(actual, predicted)\n",
    "    r2 = r2_score(actual, predicted)\n",
    "    \n",
    "    # Calculate MAPE\n",
    "    mask = actual != 0\n",
    "    mape = np.mean(np.abs((actual[mask] - predicted[mask]) / actual[mask])) * 100\n",
    "    \n",
    "    return {\n",
    "        'mse': mse,\n",
    "        'rmse': rmse,\n",
    "        'mae': mae,\n",
    "        'r2': r2,\n",
    "        'mape': mape\n",
    "    }\n",
    "\n",
    "# Calculate the metrics on aligned data\n",
    "lstm_metrics_aligned = calculate_metrics(actual_aligned, lstm_aligned)\n",
    "arima_metrics_aligned = calculate_metrics(actual_aligned, arima_aligned)\n",
    "prophet_metrics_aligned = calculate_metrics(actual_aligned, prophet_aligned)\n",
    "\n",
    "# Create the comparison table\n",
    "ts_comparison_data = {\n",
    "    'Model': ['LSTM', 'ARIMA', 'Prophet'],\n",
    "    'MSE': [lstm_metrics_aligned['mse'], arima_metrics_aligned['mse'], prophet_metrics_aligned['mse']],\n",
    "    'RMSE': [lstm_metrics_aligned['rmse'], arima_metrics_aligned['rmse'], prophet_metrics_aligned['rmse']],\n",
    "    'MAE': [lstm_metrics_aligned['mae'], arima_metrics_aligned['mae'], prophet_metrics_aligned['mae']],\n",
    "    'R²': [lstm_metrics_aligned['r2'], arima_metrics_aligned['r2'], prophet_metrics_aligned['r2']],\n",
    "    'MAPE (%)': [lstm_metrics_aligned['mape'], arima_metrics_aligned['mape'], prophet_metrics_aligned['mape']],\n",
    "    'Training Time (s)': [ts_training_time, arima_training_time, prophet_training_time],\n",
    "    'Inference Time (s)': [lstm_inference_time, arima_inference_time, prophet_inference_time]\n",
    "}\n",
    "\n",
    "ts_comparison_df = pd.DataFrame(ts_comparison_data)\n",
    "ts_comparison_df.set_index('Model', inplace=True)\n",
    "\n",
    "# Display formatted table\n",
    "print(ts_comparison_df.round(4))\n",
    "\n",
    "# Save comparison table\n",
    "ts_comparison_df.to_csv('reports/time_series_model_comparison.csv')\n",
    "\n",
    "# Visualize comparison\n",
    "metrics_to_plot = ['RMSE', 'MAE', 'MAPE (%)']\n",
    "ts_comparison_plot_data = ts_comparison_df[metrics_to_plot].reset_index()\n",
    "ts_comparison_plot_data_melted = pd.melt(ts_comparison_plot_data, id_vars='Model', var_name='Metric', value_name='Value')\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='Metric', y='Value', hue='Model', data=ts_comparison_plot_data_melted)\n",
    "plt.title('Time Series Model Comparison')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend(title='Model')\n",
    "plt.tight_layout()\n",
    "plt.savefig('reports/figures/time_series_model_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Error Analysis\n",
    "\n",
    "Let's conduct a detailed error analysis to understand the limitations of our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification error analysis\n",
    "# Let's analyze the best classification model (based on results, assume Gradient Boosting)\n",
    "try:\n",
    "    error_results = error_analysis(y_test, gb_pred, X_test, feature_names)\n",
    "\n",
    "    print(\"Classification Error Analysis:\")\n",
    "    print(f\"Error rate: {error_results['error_rate']:.4f}\")\n",
    "    print(f\"Number of errors: {error_results['num_errors']}\")\n",
    "    print(f\"False positives: {error_results['num_false_positives']}\")\n",
    "    print(f\"False negatives: {error_results['num_false_negatives']}\")\n",
    "\n",
    "    # Analyze feature statistics for errors vs. correct predictions\n",
    "    if 'feature_stats' in error_results:\n",
    "        try:\n",
    "            top_features = list(feature_importance['Feature'].head(10))\n",
    "            error_stats = pd.DataFrame()\n",
    "            \n",
    "            for feature in top_features:\n",
    "                if feature in error_results['feature_stats']:\n",
    "                    stats = error_results['feature_stats'][feature]\n",
    "                    error_stats[feature] = [\n",
    "                        stats['correct_mean'],\n",
    "                        stats['error_mean'],\n",
    "                        stats['fp_mean'],\n",
    "                        stats['fn_mean']\n",
    "                    ]\n",
    "            \n",
    "            error_stats.index = ['Correct', 'Error', 'False Positive', 'False Negative']\n",
    "            print(\"\\nFeature statistics for top features:\")\n",
    "            print(error_stats.round(3))\n",
    "        except Exception as e:\n",
    "            print(f\"Error in feature statistics analysis: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error in classification error analysis: {e}\")\n",
    "    \n",
    "    # Save error analysis\n",
    "    error_stats.to_csv('reports/classification_error_analysis.csv')\n",
    "    \n",
    "    # Visualize for a few key features\n",
    "    selected_features = top_features[:3]  # Select top 3 features\n",
    "    \n",
    "    plt.figure(figsize=(15, 5))\n",
    "    for i, feature in enumerate(selected_features):\n",
    "        plt.subplot(1, 3, i+1)\n",
    "        data = error_stats[feature].dropna()\n",
    "        sns.barplot(x=data.index, y=data.values)\n",
    "        plt.title(feature)\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "    \n",
    "    plt.suptitle('Feature Values Across Prediction Types', y=1.05)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('reports/figures/classification_error_analysis.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the reports directory exists\n",
    "os.makedirs('reports/figures', exist_ok=True)\n",
    "\n",
    "# Debug the data structure\n",
    "print(f\"Type of test_data: {type(test_data)}\")\n",
    "print(f\"Type of test_data.index: {type(test_data.index)}\")\n",
    "print(f\"First few rows of test_data:\")\n",
    "print(test_data.head())\n",
    "\n",
    "# Check if 'date' column exists in test_data\n",
    "if 'date' in test_data.columns:\n",
    "    print(\"Found 'date' column - converting to DatetimeIndex\")\n",
    "    # Convert the date column to datetime if it's not already\n",
    "    test_data['date'] = pd.to_datetime(test_data['date'])\n",
    "    # Set date as index\n",
    "    test_data = test_data.set_index('date')\n",
    "    print(f\"New index type: {type(test_data.index)}\")\n",
    "elif any(col.lower().startswith(('date', 'time', 'dt')) for col in test_data.columns):\n",
    "    # Try to find a date-like column\n",
    "    date_cols = [col for col in test_data.columns if col.lower().startswith(('date', 'time', 'dt'))]\n",
    "    if date_cols:\n",
    "        date_col = date_cols[0]\n",
    "        print(f\"Found potential date column: '{date_col}' - converting to DatetimeIndex\")\n",
    "        test_data[date_col] = pd.to_datetime(test_data[date_col])\n",
    "        test_data = test_data.set_index(date_col)\n",
    "        print(f\"New index type: {type(test_data.index)}\")\n",
    "\n",
    "# If no date column exists, create a dummy date index\n",
    "if not isinstance(test_data.index, pd.DatetimeIndex):\n",
    "    print(\"No date column found. Creating a dummy DatetimeIndex\")\n",
    "    \n",
    "    # Create a date range starting from a reasonable date\n",
    "    # Adjust the frequency based on your data (e.g., 'M' for monthly, 'W' for weekly)\n",
    "    date_range = pd.date_range(start='2023-01-01', periods=len(test_data), freq='M')\n",
    "    \n",
    "    # Set the created date range as the index\n",
    "    test_data = test_data.copy()  # Create a copy to avoid modifying the original\n",
    "    test_data.index = date_range\n",
    "    print(f\"Created DatetimeIndex with frequency 'M' (monthly)\")\n",
    "    print(f\"New index type: {type(test_data.index)}\")\n",
    "\n",
    "# Time series error analysis by period\n",
    "try:\n",
    "    time_periods = test_data.index.to_period('M').unique()\n",
    "    period_errors = {}\n",
    "\n",
    "    for period in time_periods:\n",
    "        period_mask = test_data.index.to_period('M') == period\n",
    "        period_actuals = test_data.loc[period_mask, 'crime_count'].values\n",
    "        \n",
    "        # Make sure the mask and predictions align\n",
    "        if sum(period_mask) <= len(lstm_pred):\n",
    "            period_preds = lstm_pred[period_mask]\n",
    "            \n",
    "            # Calculate metrics\n",
    "            if len(period_actuals) == len(period_preds):\n",
    "                period_mse = mean_squared_error(period_actuals, period_preds)\n",
    "                period_rmse = np.sqrt(period_mse)\n",
    "                period_mae = mean_absolute_error(period_actuals, period_preds)\n",
    "                \n",
    "                period_errors[period.strftime('%Y-%m')] = {\n",
    "                    'RMSE': period_rmse,\n",
    "                    'MAE': period_mae,\n",
    "                    'Count': len(period_actuals)\n",
    "                }\n",
    "            else:\n",
    "                print(f\"Skipping period {period}: Length mismatch between actuals ({len(period_actuals)}) and predictions ({len(period_preds)})\")\n",
    "        else:\n",
    "            print(f\"Skipping period {period}: Mask has more True values ({sum(period_mask)}) than predictions ({len(lstm_pred)})\")\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    if period_errors:\n",
    "        period_errors_df = pd.DataFrame.from_dict(period_errors, orient='index')\n",
    "        print(\"Time Series Error Analysis by Period:\")\n",
    "        print(period_errors_df.round(3))\n",
    "\n",
    "        # Save period errors\n",
    "        period_errors_df.to_csv('reports/time_series_error_analysis.csv')\n",
    "\n",
    "        # Visualize errors by period\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        period_errors_df['RMSE'].plot(kind='bar')\n",
    "        plt.title('LSTM Model Error (RMSE) by Month')\n",
    "        plt.xlabel('Month')\n",
    "        plt.ylabel('RMSE')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('reports/figures/time_series_error_by_month.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"No valid periods found for error analysis\")\n",
    "except Exception as e:\n",
    "    print(f\"Error in time series error analysis: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "# Analyze error distribution\n",
    "try:\n",
    "    # Make sure arrays are the same length\n",
    "    min_length = min(len(lstm_pred), len(test_data['crime_count']))\n",
    "    \n",
    "    # Trim both arrays to the same length\n",
    "    lstm_pred_trim = lstm_pred[:min_length]\n",
    "    actuals_trim = test_data['crime_count'].values[:min_length]\n",
    "    \n",
    "    errors = lstm_pred_trim - actuals_trim\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(errors, kde=True)\n",
    "    plt.title('Distribution of LSTM Prediction Errors')\n",
    "    plt.xlabel('Error')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('reports/figures/time_series_error_distribution.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Add some error statistics\n",
    "    print(\"\\nError Distribution Statistics:\")\n",
    "    print(f\"Mean Error: {np.mean(errors):.4f}\")\n",
    "    print(f\"Median Error: {np.median(errors):.4f}\")\n",
    "    print(f\"Std Dev of Error: {np.std(errors):.4f}\")\n",
    "    print(f\"Min Error: {np.min(errors):.4f}\")\n",
    "    print(f\"Max Error: {np.max(errors):.4f}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error in error distribution analysis: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary and Conclusions\n",
    "\n",
    "Let's summarize the key findings from our analysis and modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a final summary table for documentation\n",
    "summary_data = {\n",
    "    'Task': ['Arrest Prediction', 'Arrest Prediction', 'Arrest Prediction', 'Crime Count Prediction', 'Crime Count Prediction', 'Crime Count Prediction'],\n",
    "    'Model': ['Random Forest', 'Gradient Boosting', 'MLP', 'LSTM', 'ARIMA', 'Prophet'],\n",
    "    'Best Metric': ['F1 Score', 'F1 Score', 'F1 Score', 'RMSE', 'RMSE', 'RMSE'],\n",
    "    'Value': [rf_metrics['f1_score'], gb_metrics['f1_score'], mlp_metrics['f1_score'], lstm_metrics['rmse'], arima_rmse, prophet_rmse],\n",
    "    'Training Time (s)': ['N/A', 'N/A', training_time, ts_training_time, arima_training_time, prophet_training_time],\n",
    "    'Inference Time (s)': [rf_time, gb_time, mlp_time, lstm_inference_time, arima_inference_time, prophet_inference_time]\n",
    "}\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(\"Final Model Comparison Summary:\")\n",
    "print(summary_df.round(4))\n",
    "\n",
    "# Save summary\n",
    "summary_df.to_csv('reports/model_comparison_summary.csv', index=False)\n",
    "\n",
    "# Visualize summary for arrest prediction\n",
    "arrest_models = summary_df[summary_df['Task'] == 'Arrest Prediction']\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Model', y='Value', data=arrest_models)\n",
    "plt.title('F1 Score Comparison for Arrest Prediction Models')\n",
    "plt.ylim(0, 1)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('reports/figures/arrest_prediction_summary.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Visualize summary for crime count prediction\n",
    "count_models = summary_df[summary_df['Task'] == 'Crime Count Prediction']\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Model', y='Value', data=count_models)\n",
    "plt.title('RMSE Comparison for Crime Count Prediction Models')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('reports/figures/crime_count_prediction_summary.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Feature importance summary\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='Importance', y='Feature', data=feature_importance.head(10))\n",
    "plt.title('Top 10 Feature Importances (Gradient Boosting)')\n",
    "plt.tight_layout()\n",
    "plt.savefig('reports/figures/top_features_summary.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Findings and Conclusions\n",
    "\n",
    "### Classification Task (Arrest Prediction)\n",
    "- The best performing model for arrest prediction was Gradient Boosting with an F1 score of [value from actual results].\n",
    "- Traditional ML models (Gradient Boosting and Random Forest) outperformed the deep learning approach (MLP) for this classification task.\n",
    "- The most important features for predicting arrests were [list top features from actual results].\n",
    "- The extremely low arrest rate (0.26%) posed a significant challenge, requiring resampling techniques to address class imbalance.\n",
    "- Error analysis revealed that [insights from error analysis].\n",
    "\n",
    "### Time Series Task (Crime Count Prediction)\n",
    "- The LSTM model significantly outperformed traditional time series approaches (ARIMA and Prophet) with an RMSE of [value from actual results].\n",
    "- Deep learning showed a clear advantage for the temporal pattern recognition required in crime count forecasting.\n",
    "- The model performed best during [time periods from error analysis] and struggled most with [challenging periods].\n",
    "- The error distribution analysis showed [insights from error distribution].\n",
    "\n",
    "### Overall Insights\n",
    "- Temporal patterns strongly influence both theft occurrence and arrest outcomes, with clear variations by hour of day, day of week, and season.\n",
    "- Location characteristics play a significant role, with certain areas showing consistently higher theft rates but lower arrest probabilities.\n",
    "- The extremely low arrest rate (0.26%) indicates the challenging nature of apprehending theft perpetrators, especially for pocket-picking crimes.\n",
    "- The best approach differs by task: traditional ML models excel at structured classification problems, while deep learning shows advantages for temporal pattern recognition.\n",
    "\n",
    "### Practical Implications\n",
    "- Law enforcement resources could be more effectively allocated based on predicted crime patterns.\n",
    "- The identified factors influencing arrests could inform strategies to improve case resolution rates.\n",
    "- The temporal and spatial patterns reveal optimal times and locations for preventive measures and increased vigilance.\n",
    "- The temporal and spatial patterns reveal optimal times and locations for preventive measures and increased vigilance.\n",
    "- Public awareness campaigns could be targeted to times and locations with highest theft risks.\n",
    "\n",
    "### Limitations and Future Work\n",
    "- Our analysis was limited to pocket-picking and purse-snatching incidents; extending to other theft types could provide broader insights.\n",
    "- Incorporating additional data sources, such as demographic information, economic indicators, or police patrol data, could enhance model performance.\n",
    "- Advanced spatio-temporal modeling techniques could better capture the complex interplay between location and time in theft patterns.\n",
    "- Deploying models in a real-time system with continuous updating would allow for dynamic resource allocation based on emerging patterns.\n",
    "\n",
    "In conclusion, this project demonstrates the value of applying multiple machine learning and deep learning approaches to crime data analysis. By developing models that address different aspects of the problem - arrest prediction and crime count forecasting - we've provided a comprehensive understanding of theft patterns in Chicago that can inform both law enforcement strategies and public safety initiatives."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pycaret_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
