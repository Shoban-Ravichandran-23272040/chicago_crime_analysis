{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chicago Crime Analysis - Feature Engineering and Model Preparation\n",
    "\n",
    "This notebook focuses on feature engineering, data preparation for modeling, and initial model setup based on our exploratory data analysis from notebook 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Add project root to path for imports\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "from src.data.data_loader import ChicagoCrimeDataLoader\n",
    "from src.data.data_preprocessor import CrimeDataPreprocessor\n",
    "from src.features.feature_engineering import CrimeFeatureEngineering\n",
    "\n",
    "# Set plotting style\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.rcParams.update({'font.size': 12})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the Preprocessed Data\n",
    "\n",
    "First, we'll load the theft crime data that we analyzed in notebook 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the most recent data file\n",
    "data_dir = 'data'\n",
    "data_files = [f for f in os.listdir(data_dir) if f.startswith('chicago_theft_data_') and f.endswith('.csv')]\n",
    "\n",
    "if data_files:\n",
    "    latest_file = max(data_files, key=lambda x: os.path.getctime(os.path.join(data_dir, x)))\n",
    "    print(f\"Loading data from {latest_file}\")\n",
    "    \n",
    "    df = pd.read_csv(os.path.join(data_dir, latest_file))\n",
    "    \n",
    "    # Convert date to datetime\n",
    "    if 'date' in df.columns:\n",
    "        df['date'] = pd.to_datetime(df['date'])\n",
    "        \n",
    "    print(f\"Dataset shape: {df.shape}\")\n",
    "    print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering\n",
    "\n",
    "Now we'll apply systematic feature engineering to create meaningful features for our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize feature engineering class\n",
    "feature_eng = CrimeFeatureEngineering()\n",
    "\n",
    "# Add temporal features\n",
    "print(\"Adding temporal features...\")\n",
    "df_temp = feature_eng.add_temporal_features(df)\n",
    "\n",
    "# Show new temporal features\n",
    "new_temp_features = set(df_temp.columns) - set(df.columns)\n",
    "print(f\"Added {len(new_temp_features)} temporal features:\")\n",
    "print(sorted(new_temp_features))\n",
    "\n",
    "# Preview the data with new features\n",
    "df_temp[list(new_temp_features)].head()\n",
    "\n",
    "print(df_temp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add spatial features\n",
    "print(\"Adding spatial features...\")\n",
    "df_spatial = feature_eng.add_spatial_features(df_temp)\n",
    "\n",
    "# Show new spatial features\n",
    "new_spatial_features = set(df_spatial.columns) - set(df_temp.columns)\n",
    "print(f\"Added {len(new_spatial_features)} spatial features:\")\n",
    "print(sorted(new_spatial_features))\n",
    "\n",
    "# Preview the data with new features\n",
    "df_spatial[list(new_spatial_features)].head()\n",
    "\n",
    "print(df_spatial.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add crime-specific features\n",
    "print(\"Adding crime-specific features...\")\n",
    "df_featured = feature_eng.add_crime_specific_features(df_spatial)\n",
    "\n",
    "# Show new crime-specific features\n",
    "new_crime_features = set(df_featured.columns) - set(df_spatial.columns)\n",
    "print(f\"Added {len(new_crime_features)} crime-specific features:\")\n",
    "print(sorted(new_crime_features))\n",
    "\n",
    "# Preview the data with new features\n",
    "df_featured[list(new_crime_features)].head()#\n",
    "\n",
    "print(df_featured.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Importance Analysis\n",
    "\n",
    "Let's examine the relationship between our engineered features and the target variable (arrest) using feature importance techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Prepare data for feature importance analysis\n",
    "# Select only numeric columns and fill missing values\n",
    "numeric_cols = df_featured.select_dtypes(include=['number']).columns.tolist()\n",
    "X_numeric = df_featured[numeric_cols].fillna(0)\n",
    "\n",
    "# Convert target to binary\n",
    "if 'arrest' in df_featured.columns:\n",
    "    y = df_featured['arrest'].map({True: 1, False: 0})\n",
    "    \n",
    "    # Drop target column from features\n",
    "    X_numeric = X_numeric.drop('arrest', axis=1, errors='ignore')\n",
    "    \n",
    "    # Train a simple Random Forest for feature importance\n",
    "    rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    rf.fit(X_numeric, y)\n",
    "    \n",
    "    # Get feature importances\n",
    "    importances = rf.feature_importances_\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'Feature': X_numeric.columns,\n",
    "        'Importance': importances\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    # Plot top 15 feature importances\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.barplot(x='Importance', y='Feature', data=feature_importance.head(15))\n",
    "    plt.title('Top 15 Feature Importances for Predicting Arrests')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Preparation for Classification (Arrest Prediction)\n",
    "\n",
    "Now, let's prepare the data for our classification task of predicting whether an arrest will be made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize preprocessor\n",
    "preprocessor = CrimeDataPreprocessor()\n",
    "\n",
    "# Preprocess data for classification\n",
    "X_train, X_test, y_train, y_test, feature_names = preprocessor.preprocess_classification_data(\n",
    "    df_featured, target='arrest', test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(\"Classification data preparation:\")\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Testing set shape: {X_test.shape}\")\n",
    "print(f\"Number of features: {len(feature_names)}\")\n",
    "print(f\"Class distribution in training set: \\n{y_train.value_counts(normalize=True)}\")\n",
    "print(f\"Class distribution in testing set: \\n{y_test.value_counts(normalize=True)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Addressing Class Imbalance\n",
    "\n",
    "From our EDA, we observed that arrests are very rare (only about 0.3% of cases). Let's address this class imbalance using various techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Identify categorical columns\n",
    "categorical_cols = X_train.select_dtypes(exclude=['number']).columns\n",
    "numeric_cols = X_train.select_dtypes(include=['number']).columns\n",
    "\n",
    "# One-hot encode all categorical features\n",
    "if len(categorical_cols) > 0:\n",
    "    encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "    encoded_cats = encoder.fit_transform(X_train[categorical_cols])\n",
    "    \n",
    "    # Create encoded feature names\n",
    "    encoded_feature_names = encoder.get_feature_names_out(categorical_cols)\n",
    "    \n",
    "    # Create a DataFrame with encoded features\n",
    "    encoded_df = pd.DataFrame(encoded_cats, columns=encoded_feature_names, index=X_train.index)\n",
    "    \n",
    "    # Combine with numeric features\n",
    "    X_train_encoded = pd.concat([X_train[numeric_cols], encoded_df], axis=1)\n",
    "else:\n",
    "    X_train_encoded = X_train.copy()\n",
    "\n",
    "# Now apply SMOTE on the fully encoded dataset\n",
    "smote = SMOTE(sampling_strategy=0.8, random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train_encoded, y_train)\n",
    "\n",
    "# Apply combination of over and under sampling\n",
    "over = SMOTE(sampling_strategy=0.1, random_state=42)\n",
    "under = RandomUnderSampler(sampling_strategy=0.8, random_state=42)\n",
    "steps = [('over', over), ('under', under)]\n",
    "pipeline = Pipeline(steps=steps)\n",
    "X_train_combined, y_train_combined = pipeline.fit_resample(X_train_encoded, y_train)\n",
    "\n",
    "# Print class distribution after resampling\n",
    "print(\"Original class distribution:\")\n",
    "print(Counter(y_train))\n",
    "\n",
    "print(\"\\nAfter SMOTE oversampling:\")\n",
    "print(Counter(y_train_smote))\n",
    "\n",
    "print(\"\\nAfter combined over and under sampling:\")\n",
    "print(Counter(y_train_combined))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Preparation for Time Series (Crime Count Prediction)\n",
    "\n",
    "Now, let's prepare the data for our time series task of predicting crime counts over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess data for time series (weekly aggregation)\n",
    "ts_data_weekly = preprocessor.preprocess_time_series_data(df_featured, freq='W', seq_length=8)\n",
    "\n",
    "print(\"Time series data preparation (weekly):\")\n",
    "print(f\"X_train shape: {ts_data_weekly['X_train'].shape}\")  # [samples, sequence_length, features]\n",
    "print(f\"X_test shape: {ts_data_weekly['X_test'].shape}\")\n",
    "\n",
    "# Also prepare monthly aggregation\n",
    "ts_data_monthly = preprocessor.preprocess_time_series_data(df_featured, freq='M', seq_length=6)\n",
    "\n",
    "print(\"\\nTime series data preparation (monthly):\")\n",
    "print(f\"X_train shape: {ts_data_monthly['X_train'].shape}\")\n",
    "print(f\"X_test shape: {ts_data_monthly['X_test'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the Prepared Time Series Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot original weekly data\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(ts_data_weekly['original_data']['date'], ts_data_weekly['original_data']['crime_count'])\n",
    "plt.title('Weekly Theft Crime Counts')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Number of Crimes')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot train-test split for weekly data\n",
    "train_size = len(ts_data_weekly['X_train'])\n",
    "total_size = train_size + len(ts_data_weekly['X_test'])\n",
    "dates = ts_data_weekly['dates'][:total_size]\n",
    "original_values = ts_data_weekly['original_data']['crime_count'].values[8:8+total_size]\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(dates[:train_size], original_values[:train_size], label='Training Data')\n",
    "plt.plot(dates[train_size:], original_values[train_size:], label='Testing Data')\n",
    "plt.title('Train-Test Split for Weekly Crime Counts')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Number of Crimes')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Initial Model Setup\n",
    "\n",
    "Let's set up our initial models for both tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Classification Models (Arrest Prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def align_features(X_train, X_test):\n",
    "    \"\"\"\n",
    "    Align features between training and test datasets\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_train : pandas DataFrame\n",
    "        Training features\n",
    "    X_test : pandas DataFrame\n",
    "        Test features\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    X_train_aligned, X_test_aligned : aligned DataFrames\n",
    "    \"\"\"\n",
    "    # Identify common features\n",
    "    common_features = list(set(X_train.columns) & set(X_test.columns))\n",
    "    \n",
    "    # Print out missing features for debugging\n",
    "    train_missing = set(X_train.columns) - set(common_features)\n",
    "    test_missing = set(X_test.columns) - set(common_features)\n",
    "    \n",
    "    if train_missing:\n",
    "        print(\"Features in training set but not in test set:\")\n",
    "        print(train_missing)\n",
    "    \n",
    "    if test_missing:\n",
    "        print(\"Features in test set but not in training set:\")\n",
    "        print(test_missing)\n",
    "    \n",
    "    # Align DataFrames\n",
    "    X_train_aligned = X_train[common_features]\n",
    "    X_test_aligned = X_test[common_features]\n",
    "    \n",
    "    return X_train_aligned, X_test_aligned\n",
    "# Align features first\n",
    "X_train_combined_aligned, X_test_aligned = align_features(X_train_combined, X_test)\n",
    "\n",
    "print(\"Aligned training features shape:\", X_train_combined_aligned.shape)\n",
    "print(\"Aligned test features shape:\", X_test_aligned.shape)\n",
    "\n",
    "# Verify alignment\n",
    "print(\"\\nTraining features:\", X_train_combined_aligned.columns.tolist())\n",
    "print(\"\\nTest features:\", X_test_aligned.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "# Initialize and train models on the resampled data\n",
    "# Make sure models are initialized with the same parameters\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100, \n",
    "    max_depth=None, \n",
    "    min_samples_split=2, \n",
    "    random_state=42\n",
    ")\n",
    "gb_model = GradientBoostingClassifier(\n",
    "    n_estimators=100, \n",
    "    learning_rate=0.1, \n",
    "    max_depth=3, \n",
    "    random_state=42\n",
    ")\n",
    "# Train models on aligned features\n",
    "rf_model.fit(X_train_combined_aligned, y_train_combined)\n",
    "gb_model.fit(X_train_combined_aligned, y_train_combined)\n",
    "\n",
    "# Make predictions on aligned test data\n",
    "rf_pred = rf_model.predict(X_test_aligned)\n",
    "gb_pred = gb_model.predict(X_test_aligned)\n",
    "\n",
    "# Calculate probabilities for ROC AUC\n",
    "rf_proba = rf_model.predict_proba(X_test_aligned)[:, 1]\n",
    "gb_proba = gb_model.predict_proba(X_test_aligned)[:, 1]\n",
    "\n",
    "# Evaluate models\n",
    "print(\"\\nRandom Forest - Classification Report:\")\n",
    "print(classification_report(y_test, rf_pred))\n",
    "print(f\"ROC AUC: {roc_auc_score(y_test, rf_proba):.4f}\")\n",
    "\n",
    "print(\"\\nGradient Boosting - Classification Report:\")\n",
    "print(classification_report(y_test, gb_pred))\n",
    "print(f\"ROC AUC: {roc_auc_score(y_test, gb_proba):.4f}\")\n",
    "\n",
    "# Plot confusion matrices\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "sns.heatmap(confusion_matrix(y_test, rf_pred), annot=True, fmt='d', cmap='Blues', ax=ax1)\n",
    "ax1.set_title('Random Forest Confusion Matrix')\n",
    "ax1.set_xlabel('Predicted Label')\n",
    "ax1.set_ylabel('True Label')\n",
    "\n",
    "sns.heatmap(confusion_matrix(y_test, gb_pred), annot=True, fmt='d', cmap='Blues', ax=ax2)\n",
    "ax2.set_title('Gradient Boosting Confusion Matrix')\n",
    "ax2.set_xlabel('Predicted Label')\n",
    "ax2.set_ylabel('True Label')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Feature Importance from the Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the feature names used during model training\n",
    "# Since we used aligned features, we need to use the columns from the aligned training data\n",
    "feature_names = X_train_combined_aligned.columns.tolist()\n",
    "\n",
    "# Extract feature importances from the Gradient Boosting model\n",
    "gb_importances = gb_model.feature_importances_\n",
    "\n",
    "# Create feature importance DataFrame\n",
    "gb_feature_importance = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': gb_importances\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "# Plot top 15 feature importances\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='Importance', y='Feature', data=gb_feature_importance.head(15))\n",
    "plt.title('Top 15 Feature Importances (Gradient Boosting)')\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.ylabel('Features')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Optional: Print top 15 features and their importance\n",
    "print(\"Top 15 Most Important Features:\")\n",
    "print(gb_feature_importance.head(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Setup for Deep Learning Models\n",
    "\n",
    "Let's prepare the structures for our deep learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# MLP model architecture for classification\n",
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims=[128, 64], dropout_rate=0.3):\n",
    "        super(MLPClassifier, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        # Input layer\n",
    "        self.layers.append(nn.Linear(input_dim, hidden_dims[0]))\n",
    "        self.layers.append(nn.ReLU())\n",
    "        self.layers.append(nn.BatchNorm1d(hidden_dims[0]))\n",
    "        self.layers.append(nn.Dropout(dropout_rate))\n",
    "        \n",
    "        # Hidden layers\n",
    "        for i in range(len(hidden_dims) - 1):\n",
    "            self.layers.append(nn.Linear(hidden_dims[i], hidden_dims[i+1]))\n",
    "            self.layers.append(nn.ReLU())\n",
    "            self.layers.append(nn.BatchNorm1d(hidden_dims[i+1]))\n",
    "            self.layers.append(nn.Dropout(dropout_rate))\n",
    "        \n",
    "        # Output layer\n",
    "        self.layers.append(nn.Linear(hidden_dims[-1], 1))\n",
    "        self.layers.append(nn.Sigmoid())\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "# LSTM model architecture for time series\n",
    "class LSTMPredictor(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_size=64, num_layers=2, dropout=0.2):\n",
    "        super(LSTMPredictor, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state with zeros\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        # Forward propagate LSTM\n",
    "        out, _ = self.lstm(x, (h0, c0))  # out: batch_size, seq_length, hidden_size\n",
    "        \n",
    "        # Decode the hidden state of the last time step\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# Print model architectures\n",
    "input_dim = X_train.shape[1]\n",
    "mlp_model = MLPClassifier(input_dim=input_dim).to(device)\n",
    "print(\"MLP Model Architecture:\")\n",
    "print(mlp_model)\n",
    "\n",
    "lstm_model = LSTMPredictor().to(device)\n",
    "print(\"\\nLSTM Model Architecture:\")\n",
    "print(lstm_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Prepared Data for Modeling\n",
    "\n",
    "Finally, let's save the prepared data for use in our next notebook focused on model training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Create directory for processed data\n",
    "processed_dir = 'data/processed'\n",
    "os.makedirs(processed_dir, exist_ok=True)\n",
    "\n",
    "# Save classification data\n",
    "classification_data = {\n",
    "    'X_train': X_train_combined,\n",
    "    'X_test': X_test_aligned,\n",
    "    'y_train': y_train_combined,\n",
    "    'y_test': y_test,\n",
    "    'X_train_combined': X_train_combined_aligned,\n",
    "    'y_train_combined': y_train_combined,\n",
    "    'feature_names': feature_names\n",
    "}\n",
    "\n",
    "joblib.dump(classification_data, os.path.join(processed_dir, 'classification_data.joblib'))\n",
    "print(f\"Classification data saved to {os.path.join(processed_dir, 'classification_data.joblib')}\")\n",
    "\n",
    "# Save time series data\n",
    "joblib.dump(ts_data_weekly, os.path.join(processed_dir, 'time_series_weekly_data.joblib'))\n",
    "print(f\"Weekly time series data saved to {os.path.join(processed_dir, 'time_series_weekly_data.joblib')}\")\n",
    "\n",
    "joblib.dump(ts_data_monthly, os.path.join(processed_dir, 'time_series_monthly_data.joblib'))\n",
    "print(f\"Monthly time series data saved to {os.path.join(processed_dir, 'time_series_monthly_data.joblib')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary of Findings and Next Steps\n",
    "\n",
    "In this notebook, we have:\n",
    "\n",
    "1. Conducted feature engineering to create meaningful features for our models\n",
    "2. Analyzed feature importance to understand which factors most strongly influence arrests\n",
    "3. Prepared data for both classification (arrest prediction) and time series (crime count prediction) tasks\n",
    "4. Addressed the class imbalance issue through resampling techniques\n",
    "5. Set up initial machine learning models and evaluated their performance\n",
    "6. Prepared the architecture for deep learning models\n",
    "\n",
    "Key insights so far:\n",
    "\n",
    "- We've confirmed the severe class imbalance in the arrest data (only about 0.3% of theft cases result in arrests)\n",
    "- The most important features for predicting arrests appear to be related to [will be filled based on actual results]\n",
    "- Our initial machine learning models achieved [will be filled based on actual results]\n",
    "\n",
    "Next steps in notebook 3:\n",
    "\n",
    "1. Train and optimize our deep learning models (MLP for classification, LSTM for time series)\n",
    "2. Perform hyperparameter tuning for all models to improve performance\n",
    "3. Compare the performance of traditional machine learning vs. deep learning approaches\n",
    "4. Conduct in-depth error analysis to understand model limitations\n",
    "5. Develop final predictions and create visualizations to communicate results\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pycaret_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
